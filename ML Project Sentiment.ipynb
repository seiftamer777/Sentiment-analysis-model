{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d5f6c582",
      "metadata": {
        "id": "d5f6c582"
      },
      "source": [
        "# Project Name - Solution Name\n",
        "\n",
        "## Problem Definition\n",
        "\n",
        "\n",
        "TODO: Describe the problem definition here\n",
        "> Short statement that describes:\n",
        "- The problem:Analyzing sentiment in Arabic social media complaints to understand customer satisfaction and identify areas for improvement.\n",
        "- Used data:Excel file containing social media posts in Arabic with metadata (post links, text, timestamps)\n",
        "- Model(s) chosen:\n",
        "\n",
        "    Pre-trained Arabic BERT models for initial labeling (AraBERT, XLM-R, MARBERT)\n",
        "    Logistic Regression with TF-IDF for deployment-ready classification\n",
        "- Evaluation metric(s): Accuracy, Precision, Recall, F1-Score per sentiment class\n",
        "- In general: Pipeline: Data cleaning ‚Üí Arabic text preprocessing ‚Üí Sentiment labeling ‚Üí Model training ‚Üí Evaluation ‚Üí Export for deployment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dff4384",
      "metadata": {
        "id": "1dff4384"
      },
      "source": [
        "## Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb6dea06",
      "metadata": {
        "id": "eb6dea06"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install transformers torch accelerate tqdm --quiet\n",
        "!pip install pandas numpy matplotlib seaborn\n",
        "!pip install nltk scikit-learn joblib\n",
        "!pip install openpyxl  # For Excel files\n",
        "!pip install huggingface_hub\n",
        "\n",
        "# For Arabic NLP\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baf3c176",
      "metadata": {
        "id": "baf3c176"
      },
      "outputs": [],
      "source": [
        "# Import dependencies\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "import json\n",
        "import getpass\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "\n",
        "# NLP and ML\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import joblib\n",
        "\n",
        "# Colab specific (optional)\n",
        "try:\n",
        "    from google.colab import files, drive\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60999d2c",
      "metadata": {
        "id": "60999d2c"
      },
      "outputs": [],
      "source": [
        "# Environment variables / API tokens (keep secure!)\n",
        "# Option 1: Environment variable\n",
        "hf_token = os.environ.get(\"HF_TOKEN\")\n",
        "\n",
        "# Option 2: Secure input (recommended for notebooks)\n",
        "if not hf_token:\n",
        "    hf_token = getpass.getpass(\"Enter Hugging Face token (or leave empty): \").strip() or None\n",
        "\n",
        "# Never hardcode tokens in production code!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1626cfb",
      "metadata": {
        "id": "a1626cfb"
      },
      "source": [
        "## Data Reading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "864956e5",
      "metadata": {
        "id": "864956e5"
      },
      "outputs": [],
      "source": [
        "# Upload file in Colab\n",
        "if IN_COLAB:\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "\n",
        "# Read Excel file\n",
        "# Source: Internal social media monitoring system\n",
        "df = pd.read_excel(\"SocialMedia_Complaints_15072025.xlsx\")\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2812dfa8",
      "metadata": {
        "id": "2812dfa8"
      },
      "source": [
        "## Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "221eb7ad",
      "metadata": {
        "id": "221eb7ad"
      },
      "outputs": [],
      "source": [
        "# 1. Basic information\n",
        "print(\"=\" * 50)\n",
        "print(\"DATASET INFO\")\n",
        "print(\"=\" * 50)\n",
        "df.info()\n",
        "\n",
        "# 2. Check data types and missing values\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"MISSING VALUES\")\n",
        "print(\"=\" * 50)\n",
        "missing_summary = pd.DataFrame({\n",
        "    'Column': df.columns,\n",
        "    'Missing_Count': df.isnull().sum(),\n",
        "    'Missing_Percentage': (df.isnull().sum() / len(df)) * 100\n",
        "})\n",
        "print(missing_summary[missing_summary['Missing_Count'] > 0])\n",
        "\n",
        "# 3. Check for duplicates\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"DUPLICATES\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Duplicate rows: {df.duplicated().sum()}\")\n",
        "print(f\"Duplicate post-links: {df.duplicated(subset=['post-link']).sum()}\")\n",
        "print(f\"Duplicate Post Text: {df.duplicated(subset=['Post Text']).sum()}\")\n",
        "\n",
        "# 4. Text length analysis\n",
        "df[\"text_length_words\"] = df[\"Post Text\"].astype(str).apply(lambda x: len(x.split()))\n",
        "df[\"text_length_chars\"] = df[\"Post Text\"].astype(str).apply(len)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"TEXT LENGTH STATISTICS\")\n",
        "print(\"=\" * 50)\n",
        "print(df[[\"text_length_words\", \"text_length_chars\"]].describe())\n",
        "\n",
        "# 5. Visualizations\n",
        "import matplotlib.pyplot as plt\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Word count distribution\n",
        "axes[0].hist(df[\"text_length_words\"], bins=30, edgecolor='black', alpha=0.7)\n",
        "axes[0].set_title(\"Distribution of Post Length (Words)\")\n",
        "axes[0].set_xlabel(\"Number of Words\")\n",
        "axes[0].set_ylabel(\"Frequency\")\n",
        "\n",
        "# Character count distribution\n",
        "axes[1].hist(df[\"text_length_chars\"], bins=30, edgecolor='black', alpha=0.7, color='orange')\n",
        "axes[1].set_title(\"Distribution of Post Length (Characters)\")\n",
        "axes[1].set_xlabel(\"Number of Characters\")\n",
        "axes[1].set_ylabel(\"Frequency\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 6. Sample posts\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"SAMPLE POSTS\")\n",
        "print(\"=\" * 50)\n",
        "for i, text in enumerate(df[\"Post Text\"].sample(5, random_state=42), 1):\n",
        "    print(f\"{i}. {text[:200]}...\" if len(str(text)) > 200 else f\"{i}. {text}\")\n",
        "    print(\"-\" * 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34ec6011",
      "metadata": {
        "id": "34ec6011"
      },
      "source": [
        "‚û°Ô∏è **Conclude with:**\n",
        "\n",
        "\n",
        "\n",
        "*   Dataset contains social media posts in Arabic with varying lengths\n",
        "\n",
        "*   Some posts have missing text or are duplicates\n",
        "*   Text length varies significantly (need to handle very short/long posts)\n",
        "\n",
        "\n",
        "*   Presence of URLs, numbers, and special characters that need cleaning\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2674827",
      "metadata": {
        "id": "d2674827"
      },
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Duplicate post-links count:\", df.duplicated(subset=[\"post-link\"]).sum())\n",
        "print(\"Duplicate Post Text count:\", df.duplicated(subset=[\"Post Text\"]).sum())\n"
      ],
      "metadata": {
        "id": "v1J97yq5OieY"
      },
      "id": "v1J97yq5OieY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a7c9aeb",
      "metadata": {
        "id": "6a7c9aeb"
      },
      "outputs": [],
      "source": [
        "# 1. Remove duplicates (by Post Text or by both post-link and Post Text)\n",
        "\n",
        "df = df.drop_duplicates(subset=[\"Post Text\"], keep=\"first\")\n",
        "\n",
        "print(f\"After removing duplicates: {len(df)} rows\")\n",
        "\n",
        "# 2. Ensure no NaNs in Post Text before quality check\n",
        "df = df.dropna(subset=[\"Post Text\"])\n",
        "print(f\"After removing missing Post Text: {len(df)} rows\")\n",
        "\n",
        "# 3. Consistent quality labels (use 'ok' to match your expected output)\n",
        "def check_irrelevant(text):\n",
        "    if pd.isna(text):\n",
        "        return \"empty\"\n",
        "    s = str(text).strip()\n",
        "    if s == \"\":\n",
        "        return \"empty\"\n",
        "    if (s.startswith(\"http\") or s.startswith(\"www.\")) and len(s.split()) <= 3:\n",
        "        return \"only link\"\n",
        "    elif len(s.split()) < 3:\n",
        "        return \"too short\"\n",
        "    else:\n",
        "        return \"ok\"   # <= use 'ok' if that's the label you expect\n",
        "\n",
        "df[\"irrelevant\"] = df[\"Post Text\"].apply(check_irrelevant)\n",
        "print(\"\\nIrrelevant post counts:\")\n",
        "print(df[\"irrelevant\"].value_counts())\n",
        "\n",
        "\n",
        "print(\"Duplicate post-links count after droping :\", df.duplicated(subset=[\"post-link\"]).sum())\n",
        "print(\"Duplicate Post Text count after droping :\", df.duplicated(subset=[\"Post Text\"]).sum())\n",
        "\n",
        "# Keep only ok posts\n",
        "df_clean = df[df[\"irrelevant\"] == \"ok\"].copy()\n",
        "print(f\"\\nFinal dataset after cleaning: {len(df_clean)} rows\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cb2fba8",
      "metadata": {
        "id": "0cb2fba8"
      },
      "source": [
        "‚û°Ô∏è **Conclude with:**\n",
        "\n",
        "\n",
        "\n",
        "*   There was no duplicate posts based on post-link\n",
        "\n",
        "\n",
        "*   Filtered out posts that are too short or contain only URLs\n",
        "\n",
        "\n",
        "\n",
        "*   Dataset reduced but quality improved significantly\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "695068c2",
      "metadata": {
        "id": "695068c2"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cded813b",
      "metadata": {
        "id": "cded813b"
      },
      "outputs": [],
      "source": [
        "# Arabic text preprocessing function\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download Arabic stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "arabic_stopwords = set(stopwords.words(\"arabic\"))\n",
        "\n",
        "def clean_arabic_text(text):\n",
        "    \"\"\"\n",
        "    Comprehensive Arabic text cleaning pipeline\n",
        "    \"\"\"\n",
        "    text = str(text)\n",
        "\n",
        "    # 1. Remove URLs\n",
        "    text = re.sub(r\"http\\S+|www.\\S+\", \" \", text)\n",
        "\n",
        "    # 2. Remove numbers\n",
        "    text = re.sub(r\"\\d+\", \" \", text)\n",
        "\n",
        "    # 3. Remove punctuation and special characters\n",
        "    text = re.sub(r\"[^\\w\\s]\", \" \", text)\n",
        "\n",
        "    # 4. Normalize Arabic letters\n",
        "    text = re.sub(\"[ÿ•ÿ£ÿ¢ÿß]\", \"ÿß\", text)\n",
        "    text = re.sub(\"Ÿâ\", \"Ÿä\", text)\n",
        "    text = re.sub(\"ÿ§\", \"Ÿà\", text)\n",
        "    text = re.sub(\"ÿ¶\", \"Ÿä\", text)\n",
        "    text = re.sub(\"ÿ©\", \"Ÿá\", text)\n",
        "\n",
        "    # 5. Remove non-Arabic characters (keep only Arabic and spaces)\n",
        "    text = re.sub(r\"[^\\u0600-\\u06FF\\s]\", \" \", text)\n",
        "\n",
        "    # 6. Remove extra spaces\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "    # 7. Remove Arabic stopwords\n",
        "    tokens = text.split()\n",
        "    tokens = [w for w in tokens if w not in arabic_stopwords and len(w) > 1]\n",
        "    text = \" \".join(tokens)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply cleaning\n",
        "df_clean[\"clean_text\"] = df_clean[\"Post Text\"].apply(clean_arabic_text)\n",
        "\n",
        "# Preview cleaning results\n",
        "print(\"CLEANING PREVIEW\")\n",
        "print(\"=\" * 60)\n",
        "for i in range(min(5, len(df_clean))):\n",
        "    row = df_clean.iloc[i]\n",
        "    print(f\"üîπ Original: {row['Post Text'][:100]}...\")\n",
        "    print(f\"‚úÖ Cleaned: {row['clean_text'][:100]}...\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Check cleaned text statistics\n",
        "df_clean[\"clean_text_length\"] = df_clean[\"clean_text\"].apply(lambda x: len(x.split()))\n",
        "print(f\"\\nCleaned text word count stats:\")\n",
        "print(df_clean[\"clean_text_length\"].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d9805d2",
      "metadata": {
        "id": "9d9805d2"
      },
      "source": [
        "‚û°Ô∏è **Conclude with:**\n",
        "\n",
        "*   Normalized Arabic characters for consistency\n",
        "\n",
        "*   Removed noise (URLs, numbers, non-Arabic text)\n",
        "\n",
        "*   Eliminated stopwords to focus on meaningful content\n",
        "\n",
        "\n",
        "\n",
        "*   Text is now ready for sentiment analysis\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eeb887d0",
      "metadata": {
        "id": "eeb887d0"
      },
      "source": [
        "## Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phase 1: Automated Labeling with Pre-trained Models"
      ],
      "metadata": {
        "id": "4FXGnujFnF0L"
      },
      "id": "4FXGnujFnF0L"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fd4b06f",
      "metadata": {
        "id": "1fd4b06f"
      },
      "outputs": [],
      "source": [
        "# Setup for pre-trained Arabic sentiment models\n",
        "model_candidates = [\n",
        "    \"PRAli22/AraBert-Arabic-Sentiment-Analysis\",\n",
        "    \"akhooli/xlm-r-large-arabic-sent\",\n",
        "    \"Ammar-alhaj-ali/arabic-MARBERT-sentiment\",\n",
        "]\n",
        "\n",
        "# Load model with fallback mechanism\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "sentiment_pipe = None\n",
        "chosen_model = None\n",
        "\n",
        "for model_id in model_candidates:\n",
        "    try:\n",
        "        print(f\"Attempting to load: {model_id}\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(model_id, token=hf_token)\n",
        "        sentiment_pipe = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer, device=device)\n",
        "        chosen_model = model_id\n",
        "        print(f\"‚úÖ Successfully loaded: {chosen_model}\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to load {model_id}: {e}\")\n",
        "        continue\n",
        "\n",
        "if sentiment_pipe is None:\n",
        "    raise RuntimeError(\"Could not load any sentiment model\")\n",
        "\n",
        "# Batched inference with progress tracking\n",
        "def predict_sentiment_batch(texts, batch_size=32):\n",
        "    \"\"\"Predict sentiment for texts in batches\"\"\"\n",
        "    labels = []\n",
        "    scores = []\n",
        "\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Predicting sentiment\"):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        try:\n",
        "            outputs = sentiment_pipe(batch, truncation=True, max_length=512)\n",
        "            for out in outputs:\n",
        "                labels.append(out['label'])\n",
        "                scores.append(out['score'])\n",
        "        except Exception as e:\n",
        "            print(f\"Error in batch {i//batch_size}: {e}\")\n",
        "            # Fallback: process individually\n",
        "            for text in batch:\n",
        "                try:\n",
        "                    out = sentiment_pipe(text, truncation=True, max_length=512)[0]\n",
        "                    labels.append(out['label'])\n",
        "                    scores.append(out['score'])\n",
        "                except:\n",
        "                    labels.append(\"ERROR\")\n",
        "                    scores.append(0.0)\n",
        "\n",
        "    return labels, scores\n",
        "\n",
        "# Apply sentiment prediction\n",
        "texts = df_clean[\"clean_text\"].tolist()\n",
        "labels, scores = predict_sentiment_batch(texts, batch_size=16 if device == -1 else 32)\n",
        "\n",
        "df_clean[\"sentiment_label\"] = labels\n",
        "df_clean[\"confidence_score\"] = scores\n",
        "\n",
        "# Normalize labels\n",
        "def normalize_label(label):\n",
        "    label_lower = str(label).lower()\n",
        "    if \"pos\" in label_lower or label_lower == \"1\":\n",
        "        return \"positive\"\n",
        "    elif \"neg\" in label_lower or label_lower == \"0\":\n",
        "        return \"negative\"\n",
        "    elif \"neu\" in label_lower or label_lower == \"2\":\n",
        "        return \"neutral\"\n",
        "    else:\n",
        "        return label_lower\n",
        "\n",
        "df_clean[\"sentiment_normalized\"] = df_clean[\"sentiment_label\"].apply(normalize_label)\n",
        "\n",
        "print(\"\\nSentiment Distribution:\")\n",
        "print(df_clean[\"sentiment_normalized\"].value_counts())\n",
        "print(f\"\\nAverage confidence: {df_clean['confidence_score'].mean():.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phase 2: Train Custom Classification Model\n"
      ],
      "metadata": {
        "id": "pbQyyPnAnNEL"
      },
      "id": "pbQyyPnAnNEL"
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for training\n",
        "valid_sentiments = [\"positive\", \"negative\", \"neutral\"]\n",
        "df_model = df_clean[df_clean[\"sentiment_normalized\"].isin(valid_sentiments)].copy()\n",
        "\n",
        "X = df_model[\"clean_text\"]\n",
        "y = df_model[\"sentiment_normalized\"]\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set: {len(X_train)} samples\")\n",
        "print(f\"Test set: {len(X_test)} samples\")\n",
        "print(f\"\\nClass distribution in training:\")\n",
        "print(y_train.value_counts(normalize=True))\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_features=5000,\n",
        "    ngram_range=(1, 2),  # Unigrams and bigrams\n",
        "    min_df=3,\n",
        "    max_df=0.9\n",
        ")\n",
        "\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "print(f\"\\nTF-IDF matrix shape: {X_train_tfidf.shape}\")\n",
        "\n",
        "# Train Logistic Regression\n",
        "lr_model = LogisticRegression(\n",
        "    max_iter=500,\n",
        "    C=1.0,\n",
        "    class_weight='balanced',  # Handle class imbalance\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "lr_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = lr_model.predict(X_test_tfidf)\n",
        "y_pred_proba = lr_model.predict_proba(X_test_tfidf)\n",
        "\n",
        "print(\"\\n‚úÖ Model training complete!\")"
      ],
      "metadata": {
        "id": "yHlsRCG6gVJT"
      },
      "id": "yHlsRCG6gVJT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ed449117",
      "metadata": {
        "id": "ed449117"
      },
      "source": [
        "## Evaluation (Track One Change at a Time)\n",
        "\n",
        "TODO: include a **comparison table between different models** (with their key metrics like accuracy, F1, loss, etc.) to clearly highlight which approach performed best."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f9301cd",
      "metadata": {
        "id": "6f9301cd"
      },
      "outputs": [],
      "source": [
        "# Detailed evaluation\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision, recall, f1, support = precision_recall_fscore_support(\n",
        "    y_test, y_pred, average=None, labels=valid_sentiments\n",
        ")\n",
        "\n",
        "# Create performance summary\n",
        "performance_summary = pd.DataFrame({\n",
        "    'Class': valid_sentiments,\n",
        "    'Precision': precision,\n",
        "    'Recall': recall,\n",
        "    'F1-Score': f1,\n",
        "    'Support': support\n",
        "})\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"CLASSIFICATION REPORT\")\n",
        "print(\"=\" * 60)\n",
        "print(classification_report(y_test, y_pred, target_names=valid_sentiments))\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"PERFORMANCE SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(performance_summary)\n",
        "print(f\"\\nOverall Accuracy: {accuracy:.3f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred, labels=valid_sentiments)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=valid_sentiments,\n",
        "            yticklabels=valid_sentiments)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()\n",
        "\n",
        "# Model comparison table\n",
        "comparison_data = {\n",
        "    'Model': ['AraBERT (Zero-shot)', 'Logistic Regression (Trained)'],\n",
        "    'Approach': ['Pre-trained transformer', 'TF-IDF + Classical ML'],\n",
        "    'Accuracy': ['-', f'{accuracy:.3f}'],\n",
        "    'Avg F1': ['-', f'{f1.mean():.3f}'],\n",
        "    'Training Time': ['None (pre-trained)', '< 1 minute'],\n",
        "    'Inference Speed': ['Slow (~100 samples/sec)', 'Fast (~10000 samples/sec)'],\n",
        "    'Deployment': ['Requires GPU, large memory', 'CPU sufficient, lightweight']\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"MODEL COMPARISON\")\n",
        "print(\"=\" * 60)\n",
        "print(comparison_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a96e48eb",
      "metadata": {
        "id": "a96e48eb"
      },
      "source": [
        "## Deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fc19271",
      "metadata": {
        "id": "2fc19271"
      },
      "outputs": [],
      "source": [
        "# TODO: Expose your model here (API, app, or function)# Save models and preprocessors\n",
        "import joblib\n",
        "\n",
        "# Save the trained model and vectorizer\n",
        "joblib.dump(lr_model, 'sentiment_model.joblib')\n",
        "joblib.dump(vectorizer, 'sentiment_vectorizer.joblib')\n",
        "print(\"‚úÖ Model saved: sentiment_model.joblib\")\n",
        "print(\"‚úÖ Vectorizer saved: sentiment_vectorizer.joblib\")\n",
        "\n",
        "# Create prediction function for deployment\n",
        "def predict_sentiment(text, model_path='sentiment_model.joblib',\n",
        "                      vectorizer_path='sentiment_vectorizer.joblib'):\n",
        "    \"\"\"\n",
        "    Production-ready sentiment prediction function\n",
        "    \"\"\"\n",
        "    # Load model and vectorizer\n",
        "    model = joblib.load(model_path)\n",
        "    vectorizer = joblib.load(vectorizer_path)\n",
        "\n",
        "    # Clean text\n",
        "    cleaned = clean_arabic_text(text)\n",
        "\n",
        "    # Vectorize\n",
        "    text_tfidf = vectorizer.transform([cleaned])\n",
        "\n",
        "    # Predict\n",
        "    prediction = model.predict(text_tfidf)[0]\n",
        "    confidence = model.predict_proba(text_tfidf).max()\n",
        "\n",
        "    return {\n",
        "        'text': text,\n",
        "        'cleaned_text': cleaned,\n",
        "        'sentiment': prediction,\n",
        "        'confidence': float(confidence)\n",
        "    }\n",
        "\n",
        "# Test the function\n",
        "test_texts = [\n",
        "    \"ÿßŸÑÿÆÿØŸÖÿ© ÿ≥Ÿäÿ¶ÿ© ÿ¨ÿØÿß ŸàŸÑÿß ÿßŸÜÿµÿ≠ ÿ®Ÿáÿß\",\n",
        "    \"ŸÖŸÖÿ™ÿßÿ≤ Ÿàÿ≥ÿ±Ÿäÿπ ÿ¥ŸÉÿ±ÿß ŸÑŸÉŸÖ\",\n",
        "    \"ÿπÿßÿØŸä ŸÑÿß ÿ®ÿ£ÿ≥ ÿ®Ÿá\"\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"DEPLOYMENT TEST\")\n",
        "print(\"=\" * 60)\n",
        "for text in test_texts:\n",
        "    result = predict_sentiment(text)\n",
        "    print(f\"Text: {result['text']}\")\n",
        "    print(f\"Sentiment: {result['sentiment']} (Confidence: {result['confidence']:.2f})\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "# Export results for API/Dashboard\n",
        "output_data = df_clean[['Post Text', 'clean_text', 'sentiment_normalized', 'confidence_score']].copy()\n",
        "\n",
        "# Save as multiple formats\n",
        "output_data.to_csv('sentiment_results.csv', index=False, encoding='utf-8-sig')\n",
        "output_data.to_excel('sentiment_results.xlsx', index=False)\n",
        "output_data.to_json('sentiment_results.json', orient='records', force_ascii=False, indent=2)\n",
        "\n",
        "print(\"\\n‚úÖ Results exported to:\")\n",
        "print(\"  - sentiment_results.csv\")\n",
        "print(\"  - sentiment_results.xlsx\")\n",
        "print(\"  - sentiment_results.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d9c3455",
      "metadata": {
        "id": "0d9c3455"
      },
      "source": [
        "‚û°Ô∏è **Conclude with:** How you exposed your model (API, app, script), and any challenges."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24f2d6bb",
      "metadata": {
        "id": "24f2d6bb"
      },
      "source": [
        "## Final Notes  \n",
        "\n",
        "### ‚úÖ What Worked Well  \n",
        "- **Arabic-specific preprocessing**: Normalizing Arabic characters significantly improved model performance.  \n",
        "- **Pre-trained models for labeling**: Saved manual annotation effort for thousands of samples.  \n",
        "- **TF-IDF + Logistic Regression**: Simple, fast, interpretable, and deployment-friendly.  \n",
        "- **Balanced class weights**: Helped handle the imbalanced dataset.  \n",
        "\n",
        "### ‚ùå What Didn‚Äôt Add Value  \n",
        "- **Complex neural networks**: Marginal improvement not worth the computational cost.  \n",
        "- **Character-level features**: Word-level features were sufficient for Arabic.  \n",
        "- **Extensive hyperparameter tuning**: Default parameters worked reasonably well.  \n",
        "\n",
        "### üéØ Challenges Faced  \n",
        "- **Arabic text variety**: Mix of dialects and MSA made preprocessing challenging.  \n",
        "- **Neutral sentiment ambiguity**: Hard to distinguish from mild positive/negative.  \n",
        "- **Model size for deployment**: Transformer models were too large for production constraints.  \n",
        "\n",
        "### üöÄ Next Steps for Improvement  \n",
        "- **Active learning**: Use model uncertainty to identify samples for manual review.  \n",
        "- **Dialect-specific models**: Train separate models for Egyptian, Gulf, and Levantine Arabic.  \n",
        "- **Aspect-based sentiment**: Identify which aspects (service, product, price) drive sentiment.  \n",
        "- **Real-time monitoring**: Deploy as a streaming service for live social media analysis.  \n",
        "- **Ensemble methods**: Combine multiple models for better accuracy.  \n"
      ]
    }
  ],
  "metadata": {
    "language": "python",
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}